# ChickenDiseaseClassification-MLOPS-e2e
Follows https://www.youtube.com/watch?v=p1bfK8ZJgkE

## Steps

1) To create a fresh repo with the MLOPS template: 
```bash
python template.py
```

2) To install this project:
```bash
$conda create -n chicken python=3.10 pip
$conda activate chicken

#Make sure:
$which pip
/c/Users/manju/.conda/envs/chicken/Scripts/pip
# Failing this means pip is not using/pointed to isolated env completely and can cause package version conflict issues with random packages in your laptop.

$pip install -r requirements.txt
```

1) DataIngestion: 
A) Primary Code: The code that directly does the task(here downloading and extracting data)
- Purpose: Get raw data from source URL and place it in designated location in project workspace. 
- Tasks: Download compressed data and extracting it into specific location in workspace.
- Location: DataIngestion is part/component of project workflow, so it is present in ```src/{project_name}/components/data_ingestion.py```

B) Secondary Code: Feeds input and output formats, supports, configures, and triggers the primary code.
- Input: Present in ```config/config.yaml```. Supplies the info: a) where is the rootdir? b) where is the dataset src? c) where to download? d) where to unzip?
- Output: Define the input datatype from DataIngestion process, using DataIngestionConfig dataclass(a class that doesnt need __init__ fn.). Present in ```src/{project_name}/entity/config_entity.py```
- ConfigurationManager: Bridging code that reads ```config/config.yaml``` to get the data paths(both src and local directory paths), and create DataIngestionConfig object. Present in ```src/{project_name}/config/configuration.py```

```bash
(ROOT)
â””â”€â”€ ðŸš€ Data Ingestion Pipeline Workflow

    â”œâ”€â”€ 1ï¸âƒ£ TRIGGER (How it starts)
    â”‚   â””â”€â”€ DVC Command: `dvc repro`
    â”‚       â””â”€â”€ Reads `dvc.yaml` to find the 'data_ingestion' stage.
    â”‚
    â”œâ”€â”€ 2ï¸âƒ£ ORCHESTRATION (The Conductor)
    â”‚   â”œâ”€â”€ ðŸ“œ dvc.yaml
    â”‚   â”‚   â””â”€â”€ Defines the stage & executes the command:
    â”‚   â”‚       â””â”€â”€ `python src/cnnClassifier/pipeline/stage_01_data_ingestion.py`
    â”‚   â”‚
    â”‚   â””â”€â”€ ðŸ stage_01_data_ingestion.py
    â”‚       â”œâ”€â”€ Purpose: Manages the execution flow for this specific stage.
    â”‚       â”œâ”€â”€ Step 1: Initializes the ConfigurationManager.
    â”‚       â””â”€â”€ Step 2: Calls the Primary Code (DataIngestion component).
    â”‚
    â”œâ”€â”€ 3ï¸âƒ£ SECONDARY CODE (Configuration & Blueprint)
    â”‚   â”œâ”€â”€ ðŸ“ config/config.yaml (The Parameters)
    â”‚   â”‚   â”œâ”€â”€ Role: Stores all variables and paths.
    â”‚   â”‚   â””â”€â”€ Contains: `source_URL`, `unzip_dir`, etc.
    â”‚   â”‚
    â”‚   â”œâ”€â”€ ðŸ§± src/cnnClassifier/entity/config_entity.py (The Schema)
    â”‚   â”‚   â”œâ”€â”€ Role: Defines the structure of the configuration.
    â”‚   â”‚   â””â”€â”€ Contains: `DataIngestionConfig` dataclass.
    â”‚   â”‚
    â”‚   â””â”€â”€ ðŸŒ‰ src/cnnClassifier/config/configuration.py (The Bridge)
    â”‚       â”œâ”€â”€ Role: Reads the YAML, validates it, and creates a structured Python object.
    â”‚       â””â”€â”€ Method: `get_data_ingestion_config()` returns a `DataIngestionConfig` object.
    â”‚
    â”œâ”€â”€ 4ï¸âƒ£ PRIMARY CODE (The Engine / The "Doer")
    â”‚   â””â”€â”€ âš™ï¸ src/cnnClassifier/components/data_ingestion.py
    â”‚       â”œâ”€â”€ Class: `DataIngestion`
    â”‚       â”œâ”€â”€ Receives: The `DataIngestionConfig` object from the orchestrator.
    â”‚       â”œâ”€â”€ Action 1: `download_file()`
    â”‚       â”‚   â””â”€â”€ Uses `urllib` to download `data.zip` from the `source_URL`.
    â”‚       â””â”€â”€ Action 2: `extract_zip_file()`
    â”‚           â””â”€â”€ Uses `zipfile` to unzip `data.zip` into the `unzip_dir`.
    â”‚
    â””â”€â”€ 5ï¸âƒ£ OUTPUT (The Result)
        â””â”€â”€ ðŸ“‚ artifacts/data_ingestion/
            â”œâ”€â”€ ðŸ“„ data.zip (The downloaded file)
            â””â”€â”€ ðŸ–¼ï¸ Chicken-fecal-images/... (The extracted image folders)

```








## Workflows

1. Update config.yaml               â€¾â€¾â€¾â€¾â€¾â€¾â€¾|
2. Update secrets.yaml [Optional]          |(Update files outside src first, except dvc.yaml)
3. Update params.yaml               _______|
4. Update the entity
5. Update the configuration manager in src config(src/project_name/config/configuration.py)
6. Update the components
7. Update the pipeline 
8. Update the main.py
9. Update the dvc.yaml



### Note:
* In Python, any directory that contains an __init__.py file is considered a package.
* Within a package, individual Python files (.py files) are considered modules.
* logs folder is not created using template.py


Given/shared a dataset
1) Download the dataset zip/Url -> define your dataset paths and their datatypes (config/config.yaml)
2) Paths/parameters saved in config.yaml should be read and initialised(src/project_name/constants/__init__.py)